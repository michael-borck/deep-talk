It's a very intense time in the field. We obviously want all of the brilliant things these AI systems can do. Come up with new course for diseases, new energy sources, incredible things for humanity. That's the promise of AI. But also there are worries. If the first AI systems are built with the wrong value systems or their build unsafely, that could be also very bad. Why are it sat down with Dennis Asabas, who is a CEO of Google DeepMind, which is the engine of the company's artificial intelligence. He's in no bell prize winner. And also, in night, we discussed AGI, the future of work and how Google plans to compete in the age of AI. This is the big interview. 

Welcome to the big interview, Dennis. Thank you. Thanks for having me. So let's start talking about AGI a little here. Now, you founded DeepMind with the idea that you would solve intelligence and then use intelligence the solve everything else. And I think it was like a 20-year mission. We're like 15 years into it and you're on track. I feel like, yeah, we're pretty much dead on track, actually. As well, we'll be our estimate. That means five years away from, you know, what I guess people will call AGI. Yeah, I think in the next five to ten years, that would be my, you know, maybe 50% chance that we'll have what we'd define as AGI. Well, some of your peers are saying two years, three years. I am others say a little more, but that's really close. That's really soon. How do we know that we're that close? There's a bit of a debate going on at the moment in the field about definitions of AGI. And then of course, dependent on that. There's different predictions for when it will happen. We've been pretty consistent from the very beginning and actually Shane Leg on my co-founders and our chief scientist, you know, he helped define the term AGI back in, I think, early, you know, 2001 type of time frame. And we've always thought about it as, you know, a system that has the ability to exhibit sort of all the cognitive capabilities we have as humans. And the reason that's important, the reference to the human mind is the human mind is the only existence proof we have, maybe in the universe, the general intelligence is possible. So if you want to claim sort of general intelligence AGI, then you need to ensure that it, it generalizes to all these domains. Is when everything's filled in, all the check marks are filled in, then we have it. Yes. So I think there are missing capabilities right now. You know, that all of us have used the latest sort of LLMs and chat bots, well, we'll know very well, like on reasoning, on planning, on memory. I don't think today's systems can invent, you know, true-do true invention, you know, true creativity. I've puffed up size new scientific theories. They're extremely useful, they're impressive, but they have holes. And actually, one of the main reasons I don't think we're at AGI yet is because of the consistency of responses. You know, in some domains, we have systems that can do international maths, Olympiad maths problems, you know, to go metal standard with our Afro-proof system. But on the other hand, these systems sometimes still trip up on high school maths, or even counting the number of letters in a word. So that to me is not what you would expect that level of sort of difference in performance across the board is, you know, not consistent enough, and therefore shows that these systems are not fully generalizing yet. But when we get it, is it then, like a phase shift, then all of a sudden things are different. All the checkmarks are checked, yeah. And we have a thing that can do everything. Are we then power in a new world? I think, you know, that again, that is debated, and it's not clear to me whether it's going to be more of a kind of incremental transition versus a step function. My guess is it looks like it's going to be more of an incremental shift. Even if you had a system like that, the physical world still operated in the physical laws, you know, factories, robots, these other things. So I'll take a while for the effects of that, you know, this sort of digital intelligence, if you like, to really impact, I think a lot of the real world things. Maybe another, you know, decade plus. But there's other theories on that too where it could come faster. Yeah, Eric Schmidt, who I think used to work at Google, has said that it's almost like a binary thing. He says if China, for instance, gets AGI, then we're cooked because if someone gets it like 10 minutes before, you know, the next guy, then you can never catch up. You know, because then it'll maintain bigger, bigger leads there. You don't buy that, I guess. I think it's an unknown. It's one of the many unknowns, which is that, you know, that's sometimes called the hard take-offs in area where, you know, the idea there is that these AGI systems, they're able to self-improve, maybe code themselves, future versions themselves, that maybe they're extremely fast at doing that. So what would be a slight lead, let's say, you know, a few days could be, could suddenly become a chasm, if that was true. But there are many other ways it could go too, where it's more incremental. Some of these self-improvement things are not able to kind of accelerate in that way. Then, you know, being around the same time would not make much difference. But it's important. I mean, these issues are in the geopolitical issues. I think the systems that are being built, they will have some imprint of the values and the kind of norms of the designers and the culture that they were embedded in. So, you know, I think it is important these kinds of international questions. So when you build AGI at Google, you know, you have that in mind, you feel competitive and imperative to, in case that's true. Oh my god, we'd better be first. It's a very intense time at the moment in the field. As everyone knows, so many resources going into lots of pressures, lots of things that that need to be researched. And there's sort of lots of different types of pressures going on. We obviously, one, all of the brilliant things that these AGI systems can do, you know, I think eventually we'll be able to meet, you know, advanced medicine and science with it. Like, we've done with AlphaFold, come up with new course for diseases, new energy sources, incredible things for humanity. That's the promise of AGI. But also, there are worries, both in terms of, you know, if the first AGI systems are built with the wrong value systems, more their bill unsafely, that could be also very bad. And, you know, there are at least two risks that I worry a lot about. One is bad actors, in-whether some individuals or road nations, repurposing general purpose AGI technology for harmful ends. And then the second one is, obviously, the technical risk of AGI itself, as it gets more and more powerful, more more gentic, can we make sure the guard wells are safe around it, they can't be circumvented. And that interacts with this idea of, you know, what are the first systems that are built by humanity going to be like? There's commercial imperative, there's national imperative and there's a safety aspect to worry about, you know, who's in the lead and where those projects are. A few years ago, the companies were saying, please regulate us, we need regulation. And now, in the US, at least, the current administration seems less interested in putting regulations on AI than accelerating it. So we can beat the Chinese. Are you still asking for regulation? Do you think that's a miss on our part? I think, you know, I'm not being consistent in this. I think there are these, you know, other geopolitical sort of overlays that have to be taken into account. And the worlds are very different place to, you know, how it was five years ago in many dimensions. But there's also, you know, I think the idea of smart regulation that makes sense around these increasingly powerful systems, I think it's going to be important. I continue to believe that. I think, though, and I've been consistent on this as well, it sort of needs to be international, which looks hard in the moment in the way the world is working. Because these systems, you know, they're going to affect everyone and their digital systems. So, you know, if you sort of restrict it in one area, that doesn't really help in terms of the overall safety of these systems, getting built, you know, for the world. And as a society, so that's the bigger problem I think is some kind of international cooperation or collaboration, I think is what's required. And then smart regulation, nimble regulation that moves as the knowledge about the research becomes, you know, better and better. Whenever, ever reach a point for you where you would feel, man, we're not putting the guard rails in, you know, we're competing that we really have to stop or you can't get them to get involved in that. I think that a lot of the leaders of the main labs, at least the Western labs, you know, we do, there's a small number of them and we do all know each other and talk to each other regularly and a lot of the lead researchers do. The problem is is that it's not clear we have the right definitions to agree when that point ends. Like today's systems, although they're, you know, they're impressive as we discussed earlier, they're also very flawed. And I don't think today systems are posing any sort of existential risk. But so it's still theoretical. But the problem is that a lot of unknowns, we don't know how fast those will come and we don't know how risky they will be. But in my view, when there are so many unknowns, then one, I'm optimistic will overcome them. At least technically, I think the geopolitical questions could be actually end up being trickier, given enough time and enough care and thought for us, you know, sort of using the scientific method as we, you know, approach this AGI point. It makes perfect sense. But on the other hand, if that time frame is there, we just don't have much time. You know, we don't. We don't know much time. I mean, we're increasingly putting resources into security and things like cyber and also research into controllability and understanding of these systems, sometimes called mechanistic interpretability. You know, there's a lot of different sub-bronches of AGI. That's why I want to get to my dream. Yeah, that being invested in and I think even more needs to happen. And then at the same time, we need to also have societal debates more about institutional building, how do we want governance to work, how we're going to get international agreement, at least on some basic principles around how these systems are used and deployed and and also built. What about the effect on work on the marketplace? Yeah. How much do you feel the AGI is going to change people's jobs? You know, the way jobs are distributed and the workforce. I don't think we've seen my views and if you talk to economists, they feel like there's not much has changed yet. You know, people are finding these tools useful, certainly in certain domains like things like alpha-fold, many, many scientists are using it to accelerate their work. So it seems to be additive at the moment. We'll see what happens over the next five, ten years. I think there's going to be a lot of change with the jobs you world. But I think as in the past, what generally tends to happen is new jobs are created. They're actually better that utilize these tools or new technologies, they're working with the internet, they're working with mobile. We'll see if it's different this time. Obviously everyone always thinks this new one will be different and it may be it will be. But I think for the next few years, it's most likely to be, you know, we'll have these incredible tools that supercharge our productivity, make us, you know, a really useful for creative tools and actually almost make us a little bit superhuman in some ways in what we're able to produce individually. So I think there's going to be a kind of golden era of the next period of what we're able to do. Well, if AGI can do everything humans can do then it would seem that they could do the new jobs, too. That's the next question about what AGI brings. But, you know, even if you have those capabilities, there's a lot of things I think we won't want to do, you know, with a machine. You know, I sometimes give this this example of doctors and nurses, you know, maybe a doctor and what the doctor does and the diagnosis, you know, one could imagine that being helped by a AI tool or even having an AI kind of doctor. On the other hand, like nursing, you know, I don't think you'd want a robot to do that. I think there's something about the human empathy aspect of that and the care and so on that's particularly humanistic. I think there's lots of examples like that where but it's going to be, you know, a different world for sure. If you're, you would talk to a graduate now. What advice would you give to keep working for the course of a lifetime? Yeah. You know, in the age of AGI. My view is currently and of course this is changing all the time with with with the technology developing. But right now, you know, if you think of the next five, ten years as being the most productive people might be tenx more productive if they are native with these tools. So I think kids today, students today, my and current would be immerse yourself in these new systems, understand them. So still, I think it's still important to study STEM and programming and other things so that you understand how their bill, maybe you can modify them yourself on top of the models that are available. There's lots of great open source models and so on. And then become, you know, incredible at things like fine tuning, system prompting, you know, system instructions, all of these additional things that anyone can do and really know how to get the most out of those tools and do it for your, you know, your research work, programming, things that you're doing on your course. And then come out of that being incredible at utilizing and those new tools for whatever it is you're going to do. Let's look a little beyond the five and ten year range. Tell me what you envision when you look at it, if you're a future in 20 years and in 30 years, if this comes about, what's the world like when aGI is everywhere? Well, if everything goes well, then we should be in an era of what I like to call sort of radical abundance. So, you know, AGI solved some of these key, what I sometimes call root node problems in the world facing society. So, a good one, examples would be curing diseases, much healthier, longer life spans, finding new energy sources, you know, whether that's optimal batteries and better, you know, we'll interpret your super-dedutters, fusion. And then if that all happens, then we should be, you know, we should be kind of era of maximum human flourishing where we travel to the stars and colonize the galaxy. That's, that's, you know, I think the beginning of that will happen in the next 20, 30 years if, if, if the next period goes well. I'm a little skeptical that I think we have an unbelievable abundance now, but we don't distribute it, you know, yeah, fairly. I think that we kind of know how to fix climate change, right? We don't need a AGI to tell us how to do it, yeah, we're not doing it. I agree with that. I think we've been, as a, as a species, a society, not good at collaborating and I think climate is a good example, but I think we're still operating humans are still operating in a zero-sum game mentality, because actually the Earth is quite fine. I'm relative to the amount of people there are now in our cities and I mean, this is the, this is why our natural habitats are being, are being destroyed and, and it's affecting, you know, wildlife and the climate and everything, and it's also partly because people are not willing to accept. We do not have to, to, to, to figure out climate, but it would require people to make sacrifices and people don't want to, but this radical abundance would mean, would be different. We would be in and, finally, like, it would feel like a non-zero-sum game. How will we get Jordan to that? Like you talk about this in example. We have vaccines and now people are, yeah, some people are being literally, let me give you a very simple example. Water access. This is going to be a huge issue in the next 10, 20 is already an issue. Countries and difference, you know, poor parts of the world, dry parts of the world, also obviously compounded by climate change. We have a solution to water access. It's desalination. It's easy. There's plenty of sea water, almost all countries have a coastline, but the problem is it's salty water. But desalination, only very rich countries, some countries do do that. Use desalination as a solution to their freshwater problem. But it costs a lot of energy. But if energy was essentially zero, there was renewable free clean energy, right, like fusion. Suddenly, you sold the water access problem. Water is, who controls a river or what you do with that, does not, it becomes, you know, much less important than it is today. I think things like water access, you know, if you run forward 20 years and there isn't a solution like that, could lead to all sort of conflicts, probably. That's the trick. What is the way it's trending, especially if you include further climate change. And there's many many examples like that. You could create rocket fuel easily because you just separate that from sea water, hydrogen and oxygen. It's just energy, again. So you feel that these problems get solved by AGI, by AI. Then we're going to, or outlook will change. And we will be, that's what I hope. That's what I hope. Yes, that's what I hope. But it would still a secondary part. So the AGI will give us the radical abundance capability, technically, like the water access. I then hope, and this is what I think we need some great philosophers or or social scientists to be involved, that should hopefully shift our mindset as a society to non-zero solve. You know, there's still the issue of, do you divide even the radical abundance fairly? Right. Of course, that's what should happen. But I think there's much more likely, once people start feeling and understanding that there is this almost limitless supply of of raw materials and energy and things like that. Do you think that driving this innovation by profit-making companies is the right way to go, or most likely, to reach that optimistic high point through that? I think it's the current capitalism, or, you know, is the current or the Western sort of democratic kind of, you know, systems have so far been proven to be sort of the best drivers of progress. So I think that's true. My view is that once you get to that sort of stage of radical abundance and post-AGI, I think economic starts changing, even the notion of value and money. So again, I think we need, I'm not sure why economists are not working harder on this if they maybe they don't believe it's that close. Right. But, but, but if they really did that like the the AGI scientists do, then I think there's a lot of economic new economic theory that's required. You know, one final thing, I actually agree with you that this is so significant and I was going to have a huge impact. But when I write about it, I always get a lot of response from people who are really angry already about artificial intelligence and what's happening. Have you tasted that? Have you gotten that pushback and anger by a lot of people? It's almost like the industrial revolution people. Yeah. But I mean, I think at any time there's, I haven't personally see a lot of that, but I've you know, read and heard a lot about it. It's very understandable. That's all that's happened many times. You say industrial revolution when there's big change, a big revolution and I think this will be at least as big as the industrial revolution, probably a lot bigger. That's surprising. There's unknowns, it's scary, things will change, but on the other hand, when I talk to people about the passion and why I'm building AI, which is to advance science and medicine and understanding of the world around us. And then I explain to people and I've demonstrated, it's not just talk. Here's our for fold, you know, no bell prize winning breakthrough, help with medicine and drug discovery. We're doing this with our omorphic now to extend it into drug discovery and we can cure diseases, how diseases that might be afflicting your family, suddenly people are like, well, that's of course what we need that. It will be immoral not to have that if that's within our grasp. And the same with climate and energy, you know, many of the big societal problems. It's not like, you know, we know we've talked about there's many big challenges facing society today. And I often say I would be very worried about our future. If I didn't know something as revolutionary as AI was coming down the line to help with those other challenges. Of course, it's also a challenge itself, right? But at least it's one of these challenges that can actually help with the others if we get it right. Well, I hope I, you're up to Miss and Holds out and it's justified. Thank you so much. I'm so much better. Thank you. 